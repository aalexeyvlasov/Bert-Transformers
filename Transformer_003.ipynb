{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "TEXT_COL, LABEL_COL = 'text', 'truth'\n",
    "\n",
    "def read_sst5(data_dir, colnames=[LABEL_COL, TEXT_COL]):\n",
    "    datasets = {}\n",
    "    for t in [\"train\", \"dev\", \"test\"]:\n",
    "        df = pd.read_csv(os.path.join(data_dir, f\"sst_{t}.txt\"), sep='\\t', header=None, names=colnames)\n",
    "        df[LABEL_COL] = df[LABEL_COL].str.replace('__label__', '')\n",
    "        df[LABEL_COL] = df[LABEL_COL].astype(int)   # Categorical data type for truth labels\n",
    "        df[LABEL_COL] = df[LABEL_COL] - 1  # Zero-index labels for PyTorch\n",
    "        datasets[t] = df\n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import multiprocessing\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, random_split, DataLoader\n",
    "from tqdm import tqdm\n",
    "from typing import Tuple\n",
    "\n",
    "n_cpu = multiprocessing.cpu_count()\n",
    "MAX_LENGTH = 256\n",
    "\n",
    "class TextProcessor:\n",
    "    def __init__(self, tokenizer, label2id: dict, max_length: int=512):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.label2id = label2id\n",
    "        self.max_length = max_length\n",
    "        self.clf_token = self.tokenizer.vocab['[CLS]']\n",
    "        self.pad_token = self.tokenizer.vocab['[PAD]']\n",
    "\n",
    "    def encode(self, input):\n",
    "        return list(self.tokenizer.convert_tokens_to_ids(o) for o in input)\n",
    "\n",
    "    def token2id(self, item: Tuple[str, str]):\n",
    "        \"Convert text (item[0]) to sequence of IDs and label (item[1]) to integer\"\n",
    "        assert len(item) == 2   # Need a row of text AND labels\n",
    "        label, text = item[0], item[1]\n",
    "        assert isinstance(text, str)   # Need position 1 of input to be of type(str)\n",
    "        inputs = self.tokenizer.tokenize(text)\n",
    "        # Trim or pad dataset\n",
    "        if len(inputs) >= self.max_length:\n",
    "            inputs = inputs[:self.max_length - 1]\n",
    "            ids = self.encode(inputs) + [self.clf_token]\n",
    "        else:\n",
    "            pad = [self.pad_token] * (self.max_length - len(inputs) - 1)\n",
    "            ids = self.encode(inputs) + [self.clf_token] + pad\n",
    "\n",
    "        return np.array(ids, dtype='int64'), self.label2id[label]\n",
    "\n",
    "    def process_row(self, row):\n",
    "        \"Calls the token2id method of the text processor for passing items to executor\"\n",
    "        return self.token2id((row[1][LABEL_COL], row[1][TEXT_COL]))\n",
    "\n",
    "    def create_dataloader(self,\n",
    "                          df: pd.DataFrame,\n",
    "                          batch_size: int = 32,\n",
    "                          shuffle: bool = False,\n",
    "                          valid_pct: float = None):\n",
    "        \"Process rows in pd.DataFrame using n_cpus and return a DataLoader\"\n",
    "\n",
    "        tqdm.pandas()\n",
    "        with ProcessPoolExecutor(max_workers=n_cpu) as executor:\n",
    "            result = list(\n",
    "                tqdm(executor.map(self.process_row, df.iterrows(), chunksize=8192),\n",
    "                     desc=f\"Processing {len(df)} examples on {n_cpu} cores\",\n",
    "                     total=len(df)))\n",
    "\n",
    "        features = [r[0] for r in result]\n",
    "        labels = [r[1] for r in result]\n",
    "\n",
    "        dataset = TensorDataset(torch.tensor(features, dtype=torch.long),\n",
    "                                torch.tensor(labels, dtype=torch.long))\n",
    "\n",
    "        data_loader = DataLoader(dataset,\n",
    "                                 batch_size=batch_size,\n",
    "                                 num_workers=0,\n",
    "                                 shuffle=shuffle,\n",
    "                                 pin_memory=torch.cuda.is_available())\n",
    "        return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, embed_dim, hidden_dim, num_embeddings, num_max_positions, num_heads, num_layers, dropout, causal):\n",
    "        super().__init__()\n",
    "        self.causal = causal\n",
    "        self.tokens_embeddings = nn.Embedding(num_embeddings, embed_dim)\n",
    "        self.position_embeddings = nn.Embedding(num_max_positions, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.attentions, self.feed_forwards = nn.ModuleList(), nn.ModuleList()\n",
    "        self.layer_norms_1, self.layer_norms_2 = nn.ModuleList(), nn.ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            self.attentions.append(nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout))\n",
    "            self.feed_forwards.append(nn.Sequential(nn.Linear(embed_dim, hidden_dim),\n",
    "                                                    nn.ReLU(),\n",
    "                                                    nn.Linear(hidden_dim, embed_dim)))\n",
    "            self.layer_norms_1.append(nn.LayerNorm(embed_dim, eps=1e-12))\n",
    "            self.layer_norms_2.append(nn.LayerNorm(embed_dim, eps=1e-12))\n",
    "\n",
    "    def forward(self, x, padding_mask=None):\n",
    "        \"\"\" x has shape [seq length, batch], padding_mask has shape [batch, seq length] \"\"\"\n",
    "        positions = torch.arange(len(x), device=x.device).unsqueeze(-1)\n",
    "        h = self.tokens_embeddings(x)\n",
    "        h = h + self.position_embeddings(positions).expand_as(h)\n",
    "        h = self.dropout(h)\n",
    "\n",
    "        attn_mask = None\n",
    "        if self.causal:\n",
    "            attn_mask = torch.full((len(x), len(x)), -float('Inf'), device=h.device, dtype=h.dtype)\n",
    "            attn_mask = torch.triu(attn_mask, diagonal=1)\n",
    "\n",
    "        for layer_norm_1, attention, layer_norm_2, feed_forward in zip(self.layer_norms_1, self.attentions,\n",
    "                                                                       self.layer_norms_2, self.feed_forwards):\n",
    "            h = layer_norm_1(h)\n",
    "            x, _ = attention(h, h, h, attn_mask=attn_mask, need_weights=False, key_padding_mask=padding_mask)\n",
    "            x = self.dropout(x)\n",
    "            h = x + h\n",
    "\n",
    "            h = layer_norm_2(h)\n",
    "            x = feed_forward(h)\n",
    "            x = self.dropout(x)\n",
    "            h = x + h\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TransformerWithClfHead(nn.Module):\n",
    "    def __init__(self, config, fine_tuning_config):\n",
    "        \"\"\" Transformer with a classification head. \"\"\"\n",
    "        super().__init__()\n",
    "        self.config = fine_tuning_config\n",
    "        self.transformer = Transformer(config.embed_dim, config.hidden_dim, config.num_embeddings,\n",
    "                                       config.num_max_positions, config.num_heads, config.num_layers,\n",
    "                                           fine_tuning_config.dropout, causal=not config.mlm)\n",
    "\n",
    "        self.classification_head = nn.Linear(config.embed_dim, fine_tuning_config.num_classes)\n",
    "        self.apply(self.init_weights)\n",
    "\n",
    "    def init_weights(self, module):\n",
    "        if isinstance(module, (nn.Linear, nn.Embedding, nn.LayerNorm)):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.init_range)\n",
    "        if isinstance(module, (nn.Linear, nn.LayerNorm)) and module.bias is not None:\n",
    "            module.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x, clf_tokens_mask, lm_labels=None, clf_labels=None, padding_mask=None):\n",
    "        hidden_states = self.transformer(x, padding_mask)\n",
    "\n",
    "        clf_tokens_states = (hidden_states * clf_tokens_mask.unsqueeze(-1).float()).sum(dim=0)\n",
    "        clf_logits = self.classification_head(clf_tokens_states)\n",
    "\n",
    "        if clf_labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss(ignore_index=-1)\n",
    "            loss = loss_fct(clf_logits.view(-1, clf_logits.size(-1)), clf_labels.view(-1))\n",
    "            return clf_logits, loss\n",
    "\n",
    "        return clf_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerWithClfHeadAndAdapters(nn.Module):\n",
    "    def __init__(self, config, fine_tuning_config):\n",
    "        \"\"\" Transformer with a classification head and adapters. \"\"\"\n",
    "        super().__init__()\n",
    "        self.config = fine_tuning_config\n",
    "        if fine_tuning_config.adapters_dim > 0:\n",
    "            self.transformer = TransformerWithAdapters(fine_tuning_config.adapters_dim, config.embed_dim, config.hidden_dim,\n",
    "                                                       config.num_embeddings, config.num_max_positions, config.num_heads,\n",
    "                                                       config.num_layers, fine_tuning_config.dropout, causal=not config.mlm)\n",
    "        else:\n",
    "            self.transformer = Transformer(config.embed_dim, config.hidden_dim, config.num_embeddings,\n",
    "                                           config.num_max_positions, config.num_heads, config.num_layers,\n",
    "                                           fine_tuning_config.dropout, causal=not config.mlm)\n",
    "\n",
    "        self.classification_head = nn.Linear(config.embed_dim, fine_tuning_config.num_classes)\n",
    "        self.apply(self.init_weights)\n",
    "\n",
    "    def init_weights(self, module):\n",
    "        if isinstance(module, (nn.Linear, nn.Embedding, nn.LayerNorm)):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.init_range)\n",
    "        if isinstance(module, (nn.Linear, nn.LayerNorm)) and module.bias is not None:\n",
    "            module.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x, clf_tokens_mask, lm_labels=None, clf_labels=None, padding_mask=None):\n",
    "        hidden_states = self.transformer(x, padding_mask)\n",
    "\n",
    "        clf_tokens_states = (hidden_states * clf_tokens_mask.unsqueeze(-1).float()).sum(dim=0)\n",
    "        clf_logits = self.classification_head(clf_tokens_states)\n",
    "\n",
    "        if clf_labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss(ignore_index=-1)\n",
    "            loss = loss_fct(clf_logits.view(-1, clf_logits.size(-1)), clf_labels.view(-1))\n",
    "            return clf_logits, loss\n",
    "\n",
    "        return clf_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from pytorch_transformers import BertTokenizer, cached_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"models/transformer\"\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "config = torch.load(cached_path(\"https://s3.amazonaws.com/models.huggingface.co/\"\n",
    "                                        \"naacl-2019-tutorial/model_training_args.bin\"))\n",
    "\n",
    "state_dict = torch.load(cached_path(\"https://s3.amazonaws.com/models.huggingface.co/\"\n",
    "                                    \"naacl-2019-tutorial/model_checkpoint.pth\"), map_location=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Namespace' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-e79e25e98e3d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m model = TransformerWithClfHeadAndAdapters(config[\"config\"],\n\u001b[0m\u001b[0;32m      2\u001b[0m                                           config[\"config_ft\"]).to(device)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'Namespace' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "model = TransformerWithClfHeadAndAdapters(config[\"config\"],\n",
    "                                          config[\"config_ft\"]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(state_dict)   # Load model state dict\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased', do_lower_case=False)  # Load tokenizer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
