{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import tarfile\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import logging\n",
    "import tarfile\n",
    "import urllib\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "logger = logging.getLogger()\n",
    "\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def download_url(url:str, dest:str, overwrite:bool=True, show_progress=True, \n",
    "                 chunk_size=1024*1024, timeout=4, retries=5)->None:\n",
    "    \"Download `url` to `dest` unless it exists and not `overwrite`.\"\n",
    "    dest = Path(dest)/os.path.basename(url)\n",
    "    if os.path.exists(dest) and not overwrite: \n",
    "        print(\"File already existing\")\n",
    "        return\n",
    "\n",
    "    s = requests.Session()\n",
    "    s.mount('http://',requests.adapters.HTTPAdapter(max_retries=retries))\n",
    "    u = s.get(url, stream=True, timeout=timeout)\n",
    "    try: file_size = int(u.headers[\"Content-Length\"])\n",
    "    except: show_progress = False\n",
    "    print(f\"Downloading {url}\")\n",
    "    with open(dest, 'wb') as f:\n",
    "        nbytes = 0\n",
    "        if show_progress: \n",
    "            pbar = tqdm(range(file_size), leave=False)\n",
    "        try:\n",
    "            for chunk in u.iter_content(chunk_size=chunk_size):\n",
    "                nbytes += len(chunk)\n",
    "                if show_progress: pbar.update(nbytes)\n",
    "                f.write(chunk)\n",
    "        except requests.exceptions.ConnectionError as e:\n",
    "            print(f\"Download failed after {retries} retries.\")\n",
    "            import sys;sys.exit(1)\n",
    "        finally:\n",
    "            return str(dest)\n",
    "        \n",
    "def untar(file_path, dest:str):\n",
    "    print(f\"Untar {os.path.basename(file_path)} to {dest}\")\n",
    "    with tarfile.open(file_path) as tf:\n",
    "        tf.extractall(path=str(dest))\n",
    "    os.remove(file_path)\n",
    "    return str(dest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "DATA_DIR = Path('./data').resolve()\n",
    "url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
    "\n",
    "IMDB_DIR = os.path.join(DATA_DIR, \"imdb5k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\pyconda3\\lib\\site-packages\\ipykernel_launcher.py:30: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f64fc70d3db34b2184adcfa21b7e92d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=84125825.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Untar aclImdb_v1.tar.gz to C:\\Users\\Алексей\\Desktop\\Монтаж\\Jupyter notebook\\Notebooks\\Transformer_001\\data\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Алексей\\\\Desktop\\\\Монтаж\\\\Jupyter notebook\\\\Notebooks\\\\Transformer_001\\\\data'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#file_path = download_url(url, '/tmp', overwrite=True)\n",
    "#untar(file_path, DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def clean_html(raw):\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    clean = re.sub(cleanr, '  ', raw)\n",
    "    return re.sub(' +', ' ', clean)\n",
    "\n",
    "\n",
    "def read_imdb(imdb_dir: str, text_col='text', label_col='label'):\n",
    "    \"Read imdb data to {'label', 'text'} format\"\n",
    "    imdb_dir = Path(imdb_dir)\n",
    "    data = {}\n",
    "    for t in ['train', 'test']:\n",
    "        texts, labels = [], []\n",
    "        for p in ['pos', 'neg']:\n",
    "            for file in tqdm((imdb_dir/'train'/p).glob(\"*.txt\"), desc=f'reading {t}/{p}'):\n",
    "                with open(file, 'r') as fin:\n",
    "                    text = fin.readlines()[0].replace(r'\\n', ' ')\n",
    "                    text = clean_html(text).strip()\n",
    "                    texts +=  [text]\n",
    "                    labels += [0 if p=='neg' else 1]\n",
    "        df = pd.DataFrame(\n",
    "        {label_col: labels, text_col: texts})\n",
    "        data[t] = df.sample(frac=1)\n",
    "\n",
    "    return tuple(data.values())\n",
    "\n",
    "def save_bertify(df: pd.DataFrame, fname: str):\n",
    "    # https://medium.com/swlh/a-simple-guide-on-using-bert-for-text-classification-bbf041ac8d04\n",
    "    fname = str(fname)\n",
    "    assert fname.endswith('.tsv'), \"fname has to be a tsv file!\"\n",
    "    \n",
    "    df_bert = pd.DataFrame({\n",
    "        'id': range(len(df)),\n",
    "        'label': df['label'],\n",
    "        'alpha': ['a'] * len(df),\n",
    "        'text': df['text']})\n",
    "    df_bert.to_csv(fname, sep='\\t', index=False, header=False)\n",
    "    print(f\"saved {len(df_bert)} bertified samples to {fname}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# text and label column names\n",
    "TEXT_COL = \"text\"\n",
    "LABEL_COL = \"label\"\n",
    "\n",
    "def clean_html(text: str):\n",
    "    \"remove html tags and whitespaces\"\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    text = re.sub(cleanr, '  ', text)\n",
    "    return re.sub(' +', ' ', text)\n",
    "\n",
    "def read_imdb(data_dir, max_lengths={\"train\": None, \"test\": None}):\n",
    "    datasets = {}\n",
    "    for t in [\"train\", \"test\"]:\n",
    "        df = pd.read_csv(os.path.join(data_dir, f\"imdb5k_{t}.csv\"))\n",
    "        if max_lengths.get(t) is not None:\n",
    "            df = df.sample(n=max_lengths.get(t))\n",
    "            df[TEXT_COL] = df[TEXT_COL].apply(lambda t: clean_html(t))\n",
    "        datasets[t] = df\n",
    "    return datasets    \n",
    "\n",
    "# read data\n",
    "datasets = read_imdb(IMDB_DIR)\n",
    "\n",
    "# list of labels\n",
    "labels = list(set(datasets[\"train\"][LABEL_COL].tolist()))\n",
    "\n",
    "# labels to integers mapping\n",
    "label2int = {label: i for i, label in enumerate(labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, random_split, DataLoader\n",
    "import numpy as np\n",
    "import warnings\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from typing import Tuple\n",
    "\n",
    "NUM_MAX_POSITIONS = 256\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "class TextProcessor: \n",
    "    # special tokens for classification and padding\n",
    "    CLS = '[CLS]'\n",
    "    PAD = '[PAD]'\n",
    "    \n",
    "    def __init__(self, tokenizer, label2id: dict, num_max_positions:int=512):\n",
    "        self.tokenizer=tokenizer\n",
    "        self.label2id = label2id\n",
    "        self.num_labels = len(label2id)\n",
    "        self.num_max_positions = num_max_positions     \n",
    "    \n",
    "    def process_example(self, example: Tuple[str, str]):\n",
    "        \"Convert text (example[0]) to sequence of IDs and label (example[1] to integer\"\n",
    "        assert len(example) == 2\n",
    "        label, text = example[0], example[1]\n",
    "        assert isinstance(text, str)\n",
    "        tokens = self.tokenizer.tokenize(text)\n",
    "\n",
    "        # truncate if too long\n",
    "        if len(tokens) >= self.num_max_positions:\n",
    "            tokens = tokens[:self.num_max_positions-1] \n",
    "            ids =  self.tokenizer.convert_tokens_to_ids(tokens) + [self.tokenizer.vocab[self.CLS]]\n",
    "        # pad if too short\n",
    "        else:\n",
    "            pad = [self.tokenizer.vocab[self.PAD]] * (self.num_max_positions-len(tokens)-1)\n",
    "            ids =  self.tokenizer.convert_tokens_to_ids(tokens) + [self.tokenizer.vocab[self.CLS]] + pad\n",
    "        \n",
    "        return ids, self.label2id[label]\n",
    "\n",
    "# download the 'bert-base-cased' tokenizer\n",
    "from pytorch_transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased', do_lower_case=False)\n",
    "\n",
    "# initialize a TextProcessor\n",
    "processor = TextProcessor(tokenizer, label2int, num_max_positions=NUM_MAX_POSITIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FineTuningConfig(num_classes=2, dropout=0.1, init_range=0.02, batch_size=32, lr=6.5e-05, max_norm=1.0, n_warmup=10, valid_pct=0.1, gradient_acc_steps=1, device='cpu', log_dir='./logs/', dataset_cache='./cache/dataset_cache.bin')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from collections import namedtuple\n",
    "import torch\n",
    "\n",
    "LOG_DIR = \"./logs/\"\n",
    "CACHE_DIR = \"./cache/\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "FineTuningConfig = namedtuple('FineTuningConfig',\n",
    "      field_names=\"num_classes, dropout, init_range, batch_size, lr, max_norm,\"\n",
    "                  \"n_warmup, valid_pct, gradient_acc_steps, device, log_dir, dataset_cache\")\n",
    "\n",
    "finetuning_config = FineTuningConfig(\n",
    "                2, 0.1, 0.02, BATCH_SIZE, 6.5e-5, 1.0,\n",
    "                10, 0.1, 1, device, LOG_DIR, \n",
    "                CACHE_DIR+'dataset_cache.bin')\n",
    "\n",
    "finetuning_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from multiprocessing import cpu_count\n",
    "from itertools import repeat\n",
    "\n",
    "num_cores = cpu_count()\n",
    "\n",
    "def process_row(processor, row):\n",
    "    return processor.process_example((row[1][LABEL_COL], row[1][TEXT_COL]))\n",
    "\n",
    "def create_dataloader(df: pd.DataFrame,\n",
    "                      processor: TextProcessor,\n",
    "                      batch_size: int = 32,\n",
    "                      shuffle: bool = False,\n",
    "                      valid_pct: float = None,\n",
    "                      text_col: str = \"text\",\n",
    "                      label_col: str = \"label\"):\n",
    "    \"Process rows in `df` with `num_cores` workers using `processor`.\"\n",
    "\n",
    "    with ProcessPoolExecutor(max_workers=num_cores) as executor:\n",
    "        result = list(\n",
    "            tqdm(executor.map(process_row,\n",
    "                              repeat(processor),\n",
    "                              df.iterrows(),\n",
    "                              chunksize=len(df) // 10),\n",
    "                 desc=f\"Processing {len(df)} examples on {num_cores} cores\",\n",
    "                 total=len(df)))\n",
    "\n",
    "    features = [r[0] for r in result]\n",
    "    labels = [r[1] for r in result]\n",
    "\n",
    "    dataset = TensorDataset(torch.tensor(features, dtype=torch.long),\n",
    "                            torch.tensor(labels, dtype=torch.long))\n",
    "\n",
    "    if valid_pct is not None:\n",
    "        valid_size = int(valid_pct * len(df))\n",
    "        train_size = len(df) - valid_size\n",
    "        valid_dataset, train_dataset = random_split(dataset,\n",
    "                                                    [valid_size, train_size])\n",
    "        valid_loader = DataLoader(valid_dataset,\n",
    "                                  batch_size=batch_size,\n",
    "                                  shuffle=False)\n",
    "        train_loader = DataLoader(train_dataset,\n",
    "                                  batch_size=batch_size,\n",
    "                                  shuffle=True)\n",
    "        return train_loader, valid_loader\n",
    "\n",
    "    data_loader = DataLoader(dataset,\n",
    "                             batch_size=batch_size,\n",
    "                             num_workers=0,\n",
    "                             shuffle=shuffle,\n",
    "                             pin_memory=torch.cuda.is_available())\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    \"Adopted from https://github.com/huggingface/naacl_transfer_learning_tutorial\"\n",
    "  \n",
    "    def __init__(self, embed_dim, hidden_dim, num_embeddings, num_max_positions, \n",
    "                 num_heads, num_layers, dropout, causal):\n",
    "        super().__init__()\n",
    "        self.causal = causal\n",
    "        self.tokens_embeddings = nn.Embedding(num_embeddings, embed_dim)\n",
    "        self.position_embeddings = nn.Embedding(num_max_positions, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.attentions, self.feed_forwards = nn.ModuleList(), nn.ModuleList()\n",
    "        self.layer_norms_1, self.layer_norms_2 = nn.ModuleList(), nn.ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            self.attentions.append(nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout))\n",
    "            self.feed_forwards.append(nn.Sequential(nn.Linear(embed_dim, hidden_dim),\n",
    "                                                    nn.ReLU(),\n",
    "                                                    nn.Linear(hidden_dim, embed_dim)))\n",
    "            self.layer_norms_1.append(nn.LayerNorm(embed_dim, eps=1e-12))\n",
    "            self.layer_norms_2.append(nn.LayerNorm(embed_dim, eps=1e-12))\n",
    "\n",
    "    def forward(self, x, padding_mask=None):\n",
    "        \"x, padding_mask - shape: [S, B]\"\n",
    "        positions = torch.arange(len(x), device=x.device).unsqueeze(-1)\n",
    "        h = self.tokens_embeddings(x)\n",
    "        h = h + self.position_embeddings(positions).expand_as(h)\n",
    "        h = self.dropout(h)\n",
    "        attn_mask = None\n",
    "        if self.causal:\n",
    "            attn_mask = torch.full((len(x), len(x)), -float('Inf'), device=h.device, dtype=h.dtype)\n",
    "            attn_mask = torch.triu(attn_mask, diagonal=1)\n",
    "\n",
    "        for layer_norm_1, attention, layer_norm_2, feed_forward in zip(self.layer_norms_1, self.attentions,\n",
    "                                                                       self.layer_norms_2, self.feed_forwards):\n",
    "            h = layer_norm_1(h)\n",
    "            x, _ = attention(h, h, h, attn_mask=attn_mask, need_weights=False, key_padding_mask=padding_mask)\n",
    "            x = self.dropout(x)\n",
    "            h = x + h\n",
    "            h = layer_norm_2(h)\n",
    "            x = feed_forward(h)\n",
    "            x = self.dropout(x)\n",
    "            h = x + h\n",
    "        return h\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerWithClfHead(nn.Module):\n",
    "    def __init__(self, config, fine_tuning_config):\n",
    "        \"\"\" Transformer with a classification head. \"\"\"\n",
    "        super().__init__()\n",
    "        self.config = fine_tuning_config\n",
    "        self.transformer = Transformer(config.embed_dim, config.hidden_dim, config.num_embeddings,\n",
    "                                       config.num_max_positions, config.num_heads, config.num_layers,\n",
    "                                           fine_tuning_config.dropout, causal=not config.mlm)\n",
    "\n",
    "        self.classification_head = nn.Linear(config.embed_dim, fine_tuning_config.num_classes)\n",
    "        self.apply(self.init_weights)\n",
    "\n",
    "    def init_weights(self, module):\n",
    "        if isinstance(module, (nn.Linear, nn.Embedding, nn.LayerNorm)):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.init_range)\n",
    "        if isinstance(module, (nn.Linear, nn.LayerNorm)) and module.bias is not None:\n",
    "            module.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x, clf_tokens_mask, lm_labels=None, clf_labels=None, padding_mask=None):\n",
    "        hidden_states = self.transformer(x, padding_mask)\n",
    "\n",
    "        clf_tokens_states = (hidden_states * clf_tokens_mask.unsqueeze(-1).float()).sum(dim=0)\n",
    "        clf_logits = self.classification_head(clf_tokens_states)\n",
    "\n",
    "        if clf_labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss(ignore_index=-1)\n",
    "            loss = loss_fct(clf_logits.view(-1, clf_logits.size(-1)), clf_labels.view(-1))\n",
    "            return clf_logits, loss\n",
    "\n",
    "        return clf_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████| 201626725/201626725 [02:02<00:00, 1644746.17B/s]\n"
     ]
    }
   ],
   "source": [
    "from pytorch_transformers import cached_path\n",
    "\n",
    "# download pre-trained model and config\n",
    "state_dict = torch.load(cached_path(\"https://s3.amazonaws.com/models.huggingface.co/\"\n",
    "                                    \"naacl-2019-tutorial/model_checkpoint.pth\"), map_location='cpu')\n",
    "\n",
    "config = torch.load(cached_path(\"https://s3.amazonaws.com/models.huggingface.co/\"\n",
    "                                        \"naacl-2019-tutorial/model_training_args.bin\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init model: Transformer base + classifier head\n",
    "model = TransformerWithClfHead(config=config, fine_tuning_config=finetuning_config).to(finetuning_config.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=['classification_head.weight', 'classification_head.bias'], unexpected_keys=['lm_head.weight'])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(state_dict, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ignite.engine import Engine, Events\n",
    "from ignite.metrics import RunningAverage, Accuracy \n",
    "from ignite.handlers import ModelCheckpoint\n",
    "from ignite.contrib.handlers import CosineAnnealingScheduler, PiecewiseLinear, create_lr_scheduler_with_warmup, ProgressBar\n",
    "import torch.nn.functional as F\n",
    "from pytorch_transformers.optimization import AdamW\n",
    "\n",
    "# Bert optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=finetuning_config.lr, correct_bias=False) \n",
    "\n",
    "def update(engine, batch):\n",
    "    \"update function for training\"\n",
    "    model.train()\n",
    "    inputs, labels = (t.to(finetuning_config.device) for t in batch)\n",
    "    inputs = inputs.transpose(0, 1).contiguous() # [S, B]\n",
    "    _, loss = model(inputs, \n",
    "                    clf_tokens_mask = (inputs == tokenizer.vocab[processor.CLS]), \n",
    "                    clf_labels=labels)\n",
    "    loss = loss / finetuning_config.gradient_acc_steps\n",
    "    loss.backward()\n",
    "    \n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), finetuning_config.max_norm)\n",
    "    if engine.state.iteration % finetuning_config.gradient_acc_steps == 0:\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    return loss.item()\n",
    "\n",
    "def inference(engine, batch):\n",
    "    \"update function for evaluation\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        batch, labels = (t.to(finetuning_config.device) for t in batch)\n",
    "        inputs = batch.transpose(0, 1).contiguous()\n",
    "        logits = model(inputs,\n",
    "                       clf_tokens_mask = (inputs == tokenizer.vocab[processor.CLS]),\n",
    "                       padding_mask = (batch == tokenizer.vocab[processor.PAD]))\n",
    "    return logits, labels\n",
    "\n",
    "trainer = Engine(update)\n",
    "evaluator = Engine(inference)\n",
    "\n",
    "# add metric to evaluator \n",
    "Accuracy().attach(evaluator, \"accuracy\")\n",
    "\n",
    "# add evaluator to trainer: eval on valid set after each epoch\n",
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def log_validation_results(engine):\n",
    "    evaluator.run(valid_dl)\n",
    "    print(f\"validation epoch: {engine.state.epoch} acc: {100*evaluator.state.metrics['accuracy']}\")\n",
    "          \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dl' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-6720d698b783>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# lr schedule: linearly warm-up to lr and then to zero\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m scheduler = PiecewiseLinear(optimizer, 'lr', [(0, 0.0), (finetuning_config.n_warmup, finetuning_config.lr),\n\u001b[1;32m----> 3\u001b[1;33m                                               (len(train_dl)*finetuning_config.n_epochs, 0.0)])\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_event_handler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mITERATION_STARTED\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_dl' is not defined"
     ]
    }
   ],
   "source": [
    "# lr schedule: linearly warm-up to lr and then to zero\n",
    "scheduler = PiecewiseLinear(optimizer, 'lr', [(0, 0.0), (finetuning_config.n_warmup, finetuning_config.lr),\n",
    "                                              (len(train_dl)*finetuning_config.n_epochs, 0.0)])\n",
    "trainer.add_event_handler(Events.ITERATION_STARTED, scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\pyconda3\\lib\\site-packages\\ignite\\handlers\\checkpoint.py:369: UserWarning: Argument save_interval is deprecated and should be None. Please, use events filtering instead, e.g. Events.ITERATION_STARTED(every=1000)\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# add progressbar with loss\n",
    "RunningAverage(output_transform=lambda x: x).attach(trainer, \"loss\")\n",
    "ProgressBar(persist=True).attach(trainer, metric_names=['loss'])\n",
    "\n",
    "# save checkpoints and finetuning config\n",
    "checkpoint_handler = ModelCheckpoint(finetuning_config.log_dir, 'finetuning_checkpoint', \n",
    "                                     save_interval=1, require_empty=False)\n",
    "trainer.add_event_handler(Events.EPOCH_COMPLETED, checkpoint_handler, {'imdb_model': model})\n",
    "\n",
    "# save config to logdir\n",
    "torch.save(finetuning_config, os.path.join(finetuning_config.log_dir, 'fine_tuning_args.bin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model on `train_dl`\"\n",
    "trainer.run(train_dl, max_epochs=finetuning_config.n_epochs)\n",
    "\n",
    "# save model weights\n",
    "torch.save(model.state_dict(), os.path.join(finetuning_config.log_dir, \"model_weights.pth\"))\n",
    "\n",
    "# evaluate the model on `test_dl`\"\n",
    "evaluator.run(test_dl)\n",
    "print(f\"Test accuracy: {100*evaluator.state.metrics['accuracy']:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
